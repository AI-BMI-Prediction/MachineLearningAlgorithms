{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd4e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as sk\n",
    "from sklearn.metrics import f1_score ## F1 Score 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070df1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA=torch.cuda.is_available()\n",
    "DEVICE=torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea13541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch과 Batch_size 선언\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535883df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('../optimal_data2/'+'Continous_2weeks_2day_1term.xlsx')\n",
    "df.head()\n",
    "X=df.iloc[:,[1,3,4,5,6,7]]\n",
    "y=df.iloc[:,-1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X=pd.DataFrame(X)\n",
    "\n",
    "#결과 넣을 배열\n",
    "Result=[[0 for j in range(4)] for i in range(10)]\n",
    "Count=int(322/10)*83\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59dde77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.291778</td>\n",
       "      <td>-0.455078</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>-0.075599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.291778</td>\n",
       "      <td>-0.455078</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>-0.075599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.291778</td>\n",
       "      <td>-0.455078</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>-0.075599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.291778</td>\n",
       "      <td>-0.455078</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>-0.075599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.291778</td>\n",
       "      <td>-0.455078</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>-0.075599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26721</th>\n",
       "      <td>2.437066</td>\n",
       "      <td>-0.467768</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>0.323013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26722</th>\n",
       "      <td>2.437066</td>\n",
       "      <td>-0.467768</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>0.323013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26723</th>\n",
       "      <td>2.437066</td>\n",
       "      <td>-0.467768</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>0.323013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26724</th>\n",
       "      <td>2.437066</td>\n",
       "      <td>-0.467768</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>0.323013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26725</th>\n",
       "      <td>2.437066</td>\n",
       "      <td>-0.467768</td>\n",
       "      <td>-0.197422</td>\n",
       "      <td>-0.494817</td>\n",
       "      <td>-0.948634</td>\n",
       "      <td>0.323013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26726 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5\n",
       "0      0.291778 -0.455078 -0.197422 -0.494817 -0.948634 -0.075599\n",
       "1      0.291778 -0.455078 -0.197422 -0.494817 -0.948634 -0.075599\n",
       "2      0.291778 -0.455078 -0.197422 -0.494817 -0.948634 -0.075599\n",
       "3      0.291778 -0.455078 -0.197422 -0.494817 -0.948634 -0.075599\n",
       "4      0.291778 -0.455078 -0.197422 -0.494817 -0.948634 -0.075599\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "26721  2.437066 -0.467768 -0.197422 -0.494817 -0.948634  0.323013\n",
       "26722  2.437066 -0.467768 -0.197422 -0.494817 -0.948634  0.323013\n",
       "26723  2.437066 -0.467768 -0.197422 -0.494817 -0.948634  0.323013\n",
       "26724  2.437066 -0.467768 -0.197422 -0.494817 -0.948634  0.323013\n",
       "26725  2.437066 -0.467768 -0.197422 -0.494817 -0.948634  0.323013\n",
       "\n",
       "[26726 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd7fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(6, 128)\n",
    "        self.hidden_layer1 = nn.Linear(128, 256)\n",
    "        self.hidden_layer2 = nn.Linear(256, 128)\n",
    "        self.output_layer   = nn.Linear(128,3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out =  self.relu(self.input_layer(x))\n",
    "        out =  self.relu(self.hidden_layer1(out))\n",
    "        out =  self.relu(self.hidden_layer2(out))\n",
    "        out =  self.relu(self.output_layer(out))\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4140eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DNN().to(DEVICE)\n",
    "# 옵티마이저를 정의합니다. 옵티마이저에는 model.parameters()를 지정해야 합니다.\n",
    "optimizer    = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 손실함수(loss function)을 지정합니다. Multi-Class Classification 이기 때문에 CrossEntropy 손실을 지정하였습니다.\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e3d79",
   "metadata": {},
   "source": [
    "### 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f2c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # 학습 데이터를 DEVICE의 메모리로 보냄\n",
    "        data, target=data.to(DEVICE), target.to(DEVICE)\n",
    "        #매 반복(iteration) 마다 기울기를 계산하기 위해 zero_grad() 호출\n",
    "        optimizer.zero_grad()\n",
    "        # 실제 모델의 예측값(output) 받아오기\n",
    "        output=model(data)\n",
    "        #정답 데이터와의 CrossEntropyLoss 계산\n",
    "        # 손실함수에 output, label 값을 대입하여 손실을 계산합니다.\n",
    "        loss = loss_fn(output, torch.max(target, 1)[1])\n",
    "        #기울기 계산\n",
    "        loss.backward()\n",
    "        # 계산된 Gradient를 업데이트 합니다.\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2cd05",
   "metadata": {},
   "source": [
    "### 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad328140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    # 모델을 평가 모드로 전환\n",
    "    model.eval()\n",
    "    # 필요한 변수 초기화\n",
    "    # Test과정에서의 Loss = test_loss\n",
    "    # 실제 모델의 예측이 정답과 맞은 횟수 = correct\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_list=[]\n",
    "    with torch.no_grad(): # 평가 과정에서는 기울기를 계산하지 않으므로, no_grad명시\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            \n",
    "            _, pred = output.max(dim=1)\n",
    "            pred_array = pred.tolist()\n",
    "            pred_list.append(pred_array) # confusion matrix를 위해 pred 리턴 값\n",
    "           \n",
    "            # 모든 오차 더하기\n",
    "            test_loss += loss_fn(output, torch.max(target, 1)[1]).item() * data.size(0)\n",
    "            \n",
    "            # 가장 큰 값을 가진 클래스가 모델의 예측입니다.\n",
    "            # 예측 클래스(pred)과 정답 클래스를 비교하여 일치할 경우 correct에 1을 더합니다.\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    #정확도 계산\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ae5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한 사람당 데이터 수\n",
    "Count_1=int(83*0.1)\n",
    "#한 사람당 데이터 수\n",
    "Count_2=83\n",
    "\n",
    "X_test=pd.DataFrame()\n",
    "X_train=pd.DataFrame()\n",
    "y_test=pd.DataFrame()\n",
    "y_train=pd.DataFrame()\n",
    "empty=pd.DataFrame()\n",
    "\n",
    "#결과 넣을 배열\n",
    "Result=[[0 for j in range(4)] for i in range(10)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6639e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 Train 레이블 값 분포: \n",
      " 0.0    22300\n",
      "1.0     1194\n",
      "2.0      656\n",
      "dtype: int64\n",
      "SMOTE 적용 전 Test 레이블 값 분포: \n",
      " 0.0    2290\n",
      "1.0     161\n",
      "2.0     125\n",
      "dtype: int64\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (2576, 6) (2576, 1)\n",
      "SMOTE 적용 후 Train 레이블 값 분포: \n",
      " 0.0    22300\n",
      "1.0    22300\n",
      "2.0    22300\n",
      "dtype: int64\n",
      "SMOTE 적용 후 Test 레이블 값 분포: \n",
      " 0.0    2290\n",
      "1.0     161\n",
      "2.0     125\n",
      "dtype: int64\n",
      "[1] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[2] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[3] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[4] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[5] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[6] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[7] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[8] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[9] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[10] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[11] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[12] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[13] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[14] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[15] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[16] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[17] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[18] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[19] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[20] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[21] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[22] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[23] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[24] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[25] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[26] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[27] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[28] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[29] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[30] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[31] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[32] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[33] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[34] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[35] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[36] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[37] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[38] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[39] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[40] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[41] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[42] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[43] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[44] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[45] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[46] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[47] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[48] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[49] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[50] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[51] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[52] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[53] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[54] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[55] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[56] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[57] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[58] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[59] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[60] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[61] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[62] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[63] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[64] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[65] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[66] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[67] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[68] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[69] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[70] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[71] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[72] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[73] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[74] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[75] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[76] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[77] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[78] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[79] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[80] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[81] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[82] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[83] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[84] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[85] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[86] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[87] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[88] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[89] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[90] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[91] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[92] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[93] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[94] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[95] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[96] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[97] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[98] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[99] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[100] Test Loss: 0.0000, Accuracy: 88.90%\n",
      "[0]Predict : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'listarray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]Predict : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i,predict))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#Accuracy\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m accuracy_score\u001b[38;5;241m=\u001b[39maccuracy_score(y_test, \u001b[43mlistarray\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]Accuracy : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i,accuracy_score))   \n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#f1score\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'listarray' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    X_test=empty\n",
    "    X_train=empty\n",
    "    y_test=empty\n",
    "    y_train=empty\n",
    "    for j in range(322):\n",
    "        X_temp_test=X.iloc[Count_2*j+Count_1*i:Count_2*j+Count_1*(i+1)]\n",
    "        X_test=pd.concat([X_test,X_temp_test])\n",
    "        X_temp_train=X.iloc[Count_2*j+Count_1:Count_2*(j+1)]\n",
    "        X_train=pd.concat([X_train,X_temp_train])\n",
    "        \n",
    "        y_temp_test=y.iloc[Count_2*j+Count_1*i:Count_2*j+Count_1*(i+1)]\n",
    "        y_test=pd.concat([y_test,y_temp_test])\n",
    "        y_temp_train=y.iloc[Count_2*j+Count_1:Count_2*(j+1)]\n",
    "        y_train=pd.concat([y_train,y_temp_train])\n",
    "        \n",
    "    print('SMOTE 적용 전 Train 레이블 값 분포: \\n', y_train.value_counts())\n",
    "    print('SMOTE 적용 전 Test 레이블 값 분포: \\n', y_test.value_counts())\n",
    "    \n",
    "    # SMOTE 적용\n",
    "    smote = SMOTE(random_state=0)\n",
    "    X_train, y_train = smote.fit_resample(X_train,y_train)\n",
    "#     X_test,y_test = smote.fit_resample(X_test,y_test)\n",
    "    \n",
    "    print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_test.shape, y_test.shape)\n",
    "    print('SMOTE 적용 후 Train 레이블 값 분포: \\n', y_train.value_counts())\n",
    "    print('SMOTE 적용 후 Test 레이블 값 분포: \\n', y_test.value_counts())\n",
    "\n",
    "    #모든 데이터 torch로 변환\n",
    "    X_train = torch.FloatTensor(X_train.to_numpy())\n",
    "    X_test = torch.FloatTensor(X_test.to_numpy())    \n",
    "    y_train = torch.LongTensor(y_train.to_numpy())\n",
    "    y_test = torch.LongTensor(y_test.to_numpy())\n",
    "    \n",
    "    # train_dataset, test_dataset을 구별하여 정의\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset=TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16,shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16,shuffle=False)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, train_dataloader, optimizer)\n",
    "        test_loss, test_accuracy, predict = evaluate(model, test_dataloader)\n",
    "\n",
    "        print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch, test_loss, test_accuracy))\n",
    "        \n",
    "    print(\"[{}]Predict : {}\".format(i,predict))\n",
    "    #Accuracy\n",
    "    accuracy_score=accuracy_score(y_test, predict) * 100\n",
    "    print(\"[{}]Accuracy : {}\".format(i,accuracy_score))   \n",
    "    #f1score\n",
    "    f1 = f1_score(y_test,predict, average='weighted')\n",
    "    print(\"[{}]F1score : {}\".format(i,predict))\n",
    "    #precision/recall\n",
    "    list=sk(y_test,predict,average='weighted')\n",
    "    print(\"[{}]Precision : {}\".format(i,list[0]))\n",
    "    print(\"[{}]Recall : {}\".format(i,list[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911c335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
